{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教學目標:\n",
    "    \n",
    "回顧 CNN 網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例說明:\n",
    "    \n",
    "使用 keras 預載的模型\n",
    "\n",
    "使用 keras VGG16 預訓練的權重\n",
    "\n",
    "了解預測後的結果輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "\n",
    "    回答 Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 07:57:57.073870  6204 deprecation.py:506] From C:\\Users\\KCB-SA\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.12197632 ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [13.869927    0.         15.668329   ...  0.          0.\n",
      "     0.        ]\n",
      "   [12.802714    0.          3.92749    ...  0.          2.8722355\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          9.146543   ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 5.1881638   0.          9.783469   ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [54.349503    0.         30.78097    ... 34.848347    0.\n",
      "     6.181845  ]\n",
      "   [28.61307     0.         14.652271   ... 20.92563     0.\n",
      "    40.74757   ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ... 11.985313    0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     3.7024057 ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "    20.861809  ]\n",
      "   ...\n",
      "   [ 0.          0.         27.005749   ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          5.642855   ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "    10.505592  ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  7.205148    0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          2.1934695  ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [ 0.          0.          0.         ...  0.          0.\n",
      "     0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "#載入預訓練模型\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    " # VGG 現存模型要找到一張名為elephant.jpg做處理的預設路徑\n",
    "img_path = 'elephant.jpg'\n",
    "#載入影像\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "#執行預測\n",
    "features = model.predict(x)\n",
    "print(features)\n",
    "# decode_predictions 輸出5個最高概率：(類別, 語義概念, 預測概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題:\n",
    "\n",
    "為什麼在CNNs中激活函數選用ReLU，而不用sigmoid或tanh函數？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sigmoid or tanh as an activation function tend to lead to gradiant vanishing, \n",
    "# which is a huge problem when we want to back prop.\n",
    "# (cuz gradiants are important when we want to use back propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 範例解答\n",
    "# 問題解答:\n",
    "# (1)為什麼在CNNs中激活函數選用ReLU，而不用sigmoid或tanh函數？ ==> \n",
    "# (a)引入非線性函數作為激勵函數，不再是輸入的線性組合，可以逼近任意函數 ==> \n",
    "# (b)採用ReLU激活函數，整個過程的計算量節省很多; ==> \n",
    "# (c)ReLU會使一部分神經元的輸出為0，這樣就造成了網路的稀疏性，並且減少了參數的相互依存關系，緩解了過擬合問題的發生"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
